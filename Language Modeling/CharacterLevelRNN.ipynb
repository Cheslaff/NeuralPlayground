{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVatPegZyxzc"
      },
      "source": [
        "# **Character-level Recurrent Neural Network**\n",
        "\n",
        "In this mini-project we build a Character-level RNN for text generation trained on A. Pushkin Eugene Onegin.<br>\n",
        "\n",
        "<img src=\"https://byronsmuse.wordpress.com/wp-content/uploads/2017/04/lidia-timoshenko-1903-1976-tatyana-and-onegin-years-later.jpg\" width=40% style=\"border-radius:20px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bJjqObi7yhiv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "t48M4EGL5_cN",
        "outputId": "09120f02-beba-4a77-87d3-82bb2fbbc3d3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaXql5Oo1uJ6"
      },
      "source": [
        "### **step 1. load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xJa4TS6zvsY",
        "outputId": "15a492f3-d890-4314-e81b-ff7279b19e62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "145"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = open(\"onegin.txt\", \"r\").read()\n",
        "chars = sorted(set(data))\n",
        "n_chars = len(chars)\n",
        "\n",
        "itos = {i:s for i, s in enumerate(chars)}\n",
        "stoi = {s:i for i, s in itos.items()}\n",
        "\n",
        "n_chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T1v-RE11tqf"
      },
      "source": [
        "### **step 2. rnn**\n",
        "Okay, how does RNN work?<br>\n",
        "It takes one input token and one hidden_state token at a time.<br>\n",
        "It concatenates them and passes through a Linear Layer with tanh activation.<br>\n",
        "Finally, it is passed through final to_output transformation and returned.<br>\n",
        "We optimize RNN to return expected characters.<br>\n",
        "For character level RNN with data \"hello\" input-target mapping is the following:<br>\n",
        "h -> e<br>\n",
        "e -> l<br>\n",
        "l -> l<br>\n",
        "l -> o<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PxO5deFQ1q3C"
      },
      "outputs": [],
      "source": [
        "\"\"\"As you can see it's a tiny and simple code\"\"\"\n",
        "\n",
        "class ShallowRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)  # We concatenate input and hidden\n",
        "    self.relu = nn.ReLU()\n",
        "    self.h2o = nn.Linear(hidden_size, output_size)  # Final transformation\n",
        "\n",
        "  def init_hidden(self, device):\n",
        "    return torch.zeros((1, self.hidden_size), device=device)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    # shapes are (1, input) and (1, hidden) - one hot encoded\n",
        "    inp_w_hidden = torch.cat([input, hidden], dim=1)  #We concatenate them along dim 1\n",
        "    new_hidden = self.relu(self.i2h(inp_w_hidden))\n",
        "    return self.h2o(new_hidden), new_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNpJUCJ84eI9"
      },
      "source": [
        "Okay, we have our RNN, but how do we train it?<br>\n",
        "As described above we we are working with one token at a time.<br>\n",
        "This is to say, for epoch in range epochs we extract a random chunk of text of fixed length, encode it using one_hot_encoding, so the shape is (1, n_chars).<br>\n",
        "For this block we create a dataset of format input, target with encoded chars and pass char after char sequentially.<br>\n",
        "As you can see we have a ```init_hidden()``` method in RNN for a reason. On first step we initialize it with zeros, which is a right thing to do in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nz08P1Vm8oVM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xxPTpDWx9K4e"
      },
      "outputs": [],
      "source": [
        "# Generation Code used on trained model (and during training with logs=True)\n",
        "def generate(model, input, len_, device):\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    chars = [stoi[input]]\n",
        "    h = model.init_hidden(device)\n",
        "    for i in range(len_):\n",
        "      input = F.one_hot(torch.tensor([chars[-1]]), n_chars).to(device)\n",
        "      logits, h = model(input, h)\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, 1).item()\n",
        "      chars.append(ix)\n",
        "    return \"\".join([itos[ch] for ch in chars])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "agHvq4Oz4ctB"
      },
      "outputs": [],
      "source": [
        "# I decided to make code more modular\n",
        "# To experiment more with hyperparameters\n",
        "def train_rnn(data, input_size, hidden_size, output_size, chunk_length=50,\n",
        "              epochs=5_000, lr=0.001, logs=True, optimizer=None, device=device):\n",
        "\n",
        "  # Variables set up\n",
        "  rnn = ShallowRNN(input_size, hidden_size, output_size).to(device)\n",
        "  if not optimizer:\n",
        "    optimizer = optim.Adam(rnn.parameters(), lr)\n",
        "  else:\n",
        "    optimizer = optimizer(rnn.parameters(), lr)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    rnn.train()\n",
        "    # Data Extraction + preparation\n",
        "    chunk_start_i = torch.randint(0, len(data) - chunk_length, (1,))\n",
        "    chunk = data[chunk_start_i:(chunk_start_i + chunk_length)]\n",
        "    encoded_chunk = torch.tensor([stoi[ch] for ch in chunk])\n",
        "    ohe_chunk = F.one_hot(encoded_chunk, num_classes=n_chars).to(device)\n",
        "    input = ohe_chunk[:-1].float()\n",
        "    target = ohe_chunk[1:].float()\n",
        "    hidden = rnn.init_hidden(device)\n",
        "    chunk_loss = 0.0\n",
        "    # Training\n",
        "    for input_token, target_token in zip(input, target):\n",
        "      logit, hidden = rnn(input_token.unsqueeze(0), hidden)\n",
        "      token_loss = loss_fn(logit, target_token.unsqueeze(0))\n",
        "      chunk_loss += token_loss\n",
        "    # Optimization\n",
        "    optimizer.zero_grad()\n",
        "    chunk_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 and logs:\n",
        "      print(f\"Epoch: {epoch} | Chunk Loss: {chunk_loss.item() / chunk_length}\")\n",
        "      print(\"Generated text:\")\n",
        "      print(generate(rnn, random.choice(chars), chunk_length, device))\n",
        "      print(\"=\" * 60)\n",
        "  return rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgLKizQf-bCa",
        "outputId": "9fca8141-bfea-4d33-cb7c-a08577b5170d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Chunk Loss: 4.881054992675781\n",
            "Generated text:\n",
            "oxдO2àаЕrожАk à\"бwжaэДГЭТzъ«лê6в\n",
            "айЖ\"е’SezjгOкзcцeБ\n",
            "============================================================\n",
            "Epoch: 500 | Chunk Loss: 2.9199087524414065\n",
            "Generated text:\n",
            "ыд шытом даано,\n",
            "се дия а орых нролеиялятего,\n",
            "Нал се\n",
            "============================================================\n",
            "Epoch: 1000 | Chunk Loss: 2.563825988769531\n",
            "Generated text:\n",
            "Gяь.ей . ф шаи планьгалиносгив енов ери почан,\n",
            "И . \n",
            "============================================================\n",
            "Epoch: 1500 | Chunk Loss: 2.2844252014160156\n",
            "Generated text:\n",
            "Ew больсянкай мнат\n",
            "Пребась вже берчань нетдой днай \n",
            "============================================================\n",
            "Epoch: 2000 | Chunk Loss: 2.706788330078125\n",
            "Generated text:\n",
            "80I1)\n",
            "\n",
            "XXXVI\n",
            "\n",
            "За ни —те сь. Е жет вел: кол чере, . \n",
            "============================================================\n",
            "Epoch: 2500 | Chunk Loss: 2.3051292419433596\n",
            "Generated text:\n",
            "3B2тыйный, везмине\n",
            "И кинит орялуш ойни тыят сипоньн\n",
            "============================================================\n",
            "Epoch: 3000 | Chunk Loss: 2.3529917907714846\n",
            "Generated text:\n",
            "я.\n",
            "Но о нео Тат омерпот манеросный\n",
            "Уо похомне) мати\n",
            "============================================================\n",
            "Epoch: 3500 | Chunk Loss: 2.2949143981933595\n",
            "Generated text:\n",
            "è во угдушой тазровала ,\n",
            "Не рошиц, я ко Ьинья\n",
            "Нам и\n",
            "============================================================\n",
            "Epoch: 4000 | Chunk Loss: 1.2133419799804688\n",
            "Generated text:\n",
            "8фразыт поладутель слудилсы заболья бодссилвыл! кр.\n",
            "============================================================\n",
            "Epoch: 4500 | Chunk Loss: 2.260589294433594\n",
            "Generated text:\n",
            ";\n",
            "Вестей полкость оскожану проска!\n",
            "90\n",
            "Сель, асё, бе\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "rnn_model = train_rnn(data, n_chars, 512, n_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MtFPfdH-tdJ",
        "outputId": "999bee87-d250-4baa-a069-9e1040fb7672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Он с «воерого сожда\n",
            "Изгер когда какой деворое в В лут солно блуза приять, склуя я но сожицах. Ещи за с не е сом согласимое\n",
            "спед понного коловам,\n",
            "Не олегос,»\n",
            "20XXXVII\n",
            "\n",
            "Обнять вом они я сляский волько пастеннема вис не щеет, шлянны\n",
            "\n",
            "Не хольеревы х Как фрац\n",
            "Приск чуг.;\n",
            "Заводу и протенля,\n",
            "осто скре-мох коняте:\n",
            "Ную! на но ты,\n",
            "Она потях» и скуть\n",
            "Как вспому твой надоди гоносеглясчик?\n",
            "На жавна хручейной\n",
            "За на тоспрама;\n",
            "Просты;\n",
            "Борь биетерь любуть рамнеку в толо на сужет? V... Мохосвя, потихой\n",
            "Они глебете\n"
          ]
        }
      ],
      "source": [
        "print(generate(rnn_model, \"О\", 500, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-9CIeesG7kj"
      },
      "source": [
        "**Our pipeline is not ideal, and the model is shallow, but our model learned the structure of the text!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_WpcRRdG6e7",
        "outputId": "3f1faaaa-a15e-402d-b959-18b1aa7af876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Отоин Онегин г и камой печны. Вылнум,\n",
            "И топы смулнам и пнеретшью.\n",
            "VII\n",
            " до-жила в рушин?\n",
            "Дая Голи, тск начный\n",
            "Котких претоморень\n",
            "Имь соподо ори него оли заня;\n",
            "В резечной попрывиц\n",
            "В дехой;\n",
            "ЛАй, дурозим смей;\n",
            "Яснав иленило пуюн то приятить сву к маж като в поиль бездемий, раго, Оногин Фмла морвен...\n",
            "XL.\n",
            "сутальяныц Онегин замалдя доножит собьфая,\n",
            "Проволь сечет башных тороком раккоклена там довочу тай Тамь пешех ничию3\n",
            "Нимену мородише ж ны ницишет бозва тулою можел —\n",
            "Пер:\n",
            "Я но дварко,\n",
            "Здесь ухого,\n",
            "И грух мерук Твеши! плав,\n",
            "Идень ских оне следа.\n",
            "На ет соб нами; насв!\n",
            "VIII\n",
            "\n",
            "Евга, в соф Мелетали гонъе,\n",
            "5таяе делий,\n",
            "и стжу:  душу товыла подной пол, Фркдю завном, песто доны;\n",
            "1еи Пусуга Венно сведный; и токровой, валь уланей,\n",
            "Перикнайшит, подиса, издокно) боська, брудит\n",
            "П\n",
            "родьнсеночко, любял оне  мое-ныегин\n",
            "К коятьяни обозй\n",
            "юльгрова!\n",
            "Нла песез,\n",
            "Быль походу х соды не ждооки спак?\n",
            ". . . . . ; Лобо мСе Лыбо в ни удоная.\n",
            "Дотий\n",
            "Стов зар мчестве?\n",
            "Комей, в годаль он себлой в тупновь,\n",
            "Не бля чня ж начны, \n"
          ]
        }
      ],
      "source": [
        "print(generate(rnn_model, \"О\", 1_000, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns3W4EbRHL3f"
      },
      "source": [
        "**It even mentions \"Онегин\" - main character**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_CnnDtfHSIf"
      },
      "source": [
        "### **Deep RNN**\n",
        "Okay, shallow model performs well, but what if we train a deeper RNN with higher block length? Let's see!<br>\n",
        "Deep RNN introduces multiple of hidden layers, meaning we need to keep track of all of them<br>\n",
        "\n",
        "<img src=\"https://d2l.ai/_images/deep-rnn.svg\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "R5iurwR0HIcz"
      },
      "outputs": [],
      "source": [
        "\"\"\"I keep it simple, making all of hidden_sizes equal\"\"\"\n",
        "\n",
        "class DeepRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i_h1 = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.h1_h2 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.h2_h3 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.h3_o = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def init_hidden(self, device):\n",
        "    return [torch.randn(1, self.hidden_size, device=device) for _ in range(3)]\n",
        "\n",
        "  def forward(self, input, hiddens:tuple):\n",
        "    h1, h2, h3 = hiddens\n",
        "\n",
        "    input_w_h1 = torch.cat([input, h1], dim=1)\n",
        "    new_h1 = self.relu(self.i_h1(input_w_h1))\n",
        "\n",
        "    h1_h2 = torch.cat([new_h1, h2], dim=1)\n",
        "    new_h2 = self.relu(self.h1_h2(h1_h2))\n",
        "\n",
        "    h2_h3 = torch.cat([new_h2, h3], dim=1)\n",
        "    new_h3 = self.relu(self.h2_h3(h2_h3))\n",
        "\n",
        "    h3_o = self.h3_o(new_h3)\n",
        "\n",
        "    return h3_o, (new_h1, new_h2, new_h3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NIDk9Xl1LbFd"
      },
      "outputs": [],
      "source": [
        "def generate(model, input, len_, device):\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    chars = [stoi[input]]\n",
        "    (h1, h2, h3) = model.init_hidden(device)\n",
        "    for i in range(len_):\n",
        "      input = F.one_hot(torch.tensor([chars[-1]]), n_chars).to(device)\n",
        "      logits, (h1, h2, h3) = model(input, (h1, h2, h3))\n",
        "      probs = torch.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, 1).item()\n",
        "      chars.append(ix)\n",
        "    return \"\".join([itos[ch] for ch in chars])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "_u77YMAbM-SY"
      },
      "outputs": [],
      "source": [
        "common_chars = list(\"абвгдежзиклмнопрстуфхцчшщэюя\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wkllexswLbFd"
      },
      "outputs": [],
      "source": [
        "# Sadly I didn't foresee it...\n",
        "def train_rnn(data, input_size, hidden_size, output_size, chunk_length=50,\n",
        "              epochs=5_000, lr=0.001, logs=True, optimizer=None, device=device):\n",
        "\n",
        "  # Variables set up\n",
        "  rnn = DeepRNN(input_size, hidden_size, output_size).to(device)\n",
        "  if not optimizer:\n",
        "    optimizer = optim.Adam(rnn.parameters(), lr)\n",
        "  else:\n",
        "    optimizer = optimizer(rnn.parameters(), lr)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    rnn.train()\n",
        "    # Data Extraction + preparation\n",
        "    chunk_start_i = torch.randint(0, len(data) - chunk_length, (1,))\n",
        "    chunk = data[chunk_start_i:(chunk_start_i + chunk_length)]\n",
        "    encoded_chunk = torch.tensor([stoi[ch] for ch in chunk])\n",
        "    ohe_chunk = F.one_hot(encoded_chunk, num_classes=n_chars).to(device)\n",
        "    input = ohe_chunk[:-1].float()\n",
        "    target = ohe_chunk[1:].float()\n",
        "    h1, h2, h3 = rnn.init_hidden(device)\n",
        "    chunk_loss = 0.0\n",
        "    # Training\n",
        "    for input_token, target_token in zip(input, target):\n",
        "      logit, (h1, h2, h3) = rnn(input_token.unsqueeze(0), (h1, h2, h3))\n",
        "      token_loss = loss_fn(logit, target_token.unsqueeze(0))\n",
        "      chunk_loss += token_loss\n",
        "    # Optimization\n",
        "    optimizer.zero_grad()\n",
        "    chunk_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 and logs:\n",
        "      print(f\"Epoch: {epoch} | Chunk Loss: {chunk_loss.item() / chunk_length}\")\n",
        "      print(\"Generated text:\")\n",
        "      print(generate(rnn, random.choice(common_chars), chunk_length, device))  # I decided to use good chars\n",
        "      print(\"=\" * 60)\n",
        "  return rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCmNnEJhLbFd",
        "outputId": "3a80ae6f-a4e8-491f-ca1a-6238d5f352cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Chunk Loss: 4.952339680989583\n",
            "Generated text:\n",
            "иФ\n",
            "Ll ;FэШnniиècДЦ9:Х)uпpГги5ьâнp.вçW1rY9*v7fL\".Hêь,Л1aGvp\n",
            "èйпБЦakъdсMd1JЗqЛ\n",
            "з5\"пYз4pсQhОС3lqàâEXТj-FЗmEf;iГъ5F1у8Рd7NуDГн;pш*!п98фlëХzjдоyvЗч ЧXкaрêët\n",
            "============================================================\n",
            "Epoch: 500 | Chunk Loss: 2.5578159586588542\n",
            "Generated text:\n",
            "ши горой\n",
            "Онас, ставами дору: и рашерь, нопнывнукино.\n",
            "Кыма моят дартае дло кеша чиля—\n",
            "III\n",
            "\n",
            "На качь!.\n",
            "— сностя срония итит сошел?»\n",
            "XVI\n",
            "\n",
            "Покса евнуе не те\n",
            "============================================================\n",
            "Epoch: 1000 | Chunk Loss: 2.7156876627604167\n",
            "Generated text:\n",
            "ньнохаленьийу,\n",
            "Имкан, Лький!\n",
            "XLIIII\n",
            "\n",
            "Обвыго баяту зинное,\n",
            "Вдорох,\n",
            "И глешом6\n",
            "В освну голпы — обдорил хротрыхны кумуцкой, покоти приуренлунье,\n",
            "И кажно пр\n",
            "============================================================\n",
            "Epoch: 1500 | Chunk Loss: 2.747464599609375\n",
            "Generated text:\n",
            "мм естаят.\n",
            "Онустя грягой неводель,\n",
            "Сяцы увигда чесивь.\n",
            "Зарый плодес су селцы,\n",
            "Но ни свелим мелицясь мосснятмянрень анорит отолорый-сальстем дродит. На \n",
            "============================================================\n",
            "Epoch: 2000 | Chunk Loss: 2.39908447265625\n",
            "Generated text:\n",
            "эь регда скобли нас под дде струстра.\n",
            "На ни муд;\n",
            "Малавиний отродан.\n",
            "На пылти.) Онегиний Носей мосбувов Онегин, я нак Онегость wраши».\n",
            "Ну посрятая ег па\n",
            "============================================================\n",
            "Epoch: 2500 | Chunk Loss: 2.4267454020182293\n",
            "Generated text:\n",
            "фла сень врач\n",
            "Неброшнию повець ни и пили круда меля,\n",
            "В твувирал. Ти нацулый поковой доми;\n",
            "Каку не сног кожно обом.\n",
            "Лен, визти нибвит.»\n",
            "Дня притрый от, \n",
            "============================================================\n",
            "Epoch: 3000 | Chunk Loss: 2.116390380859375\n",
            "Generated text:\n",
            "з ей пред ваго безотленет3ши тика, без крепит!\n",
            "Что доле;\n",
            "А муданном,\n",
            "И ты\n",
            "Что люжевей брапвенный сплит слушант,\n",
            "Бедеге медление от нарые пной\n",
            "Межно бых\n",
            "============================================================\n",
            "Epoch: 3500 | Chunk Loss: 2.2198142496744793\n",
            "Generated text:\n",
            "ш ко, мяла взарина. шильом на писаним,\n",
            "Его рамянать\n",
            "Трязенья была хорь, как гроз,\n",
            "Хоть то птому в лей,\n",
            "Татьяна. Мыкождини\n",
            "И з сем не в молпает поэке сл\n",
            "============================================================\n",
            "Epoch: 4000 | Chunk Loss: 1.9563724772135416\n",
            "Generated text:\n",
            "лСрлилось;\n",
            "Ну заряный,\n",
            "И дут не вапишь; я волнипл.\n",
            "\n",
            "Не увсел, и кондали порая!\n",
            "Прямых звить,\n",
            "Медушне таши тумей,\n",
            "Ф рожены, дне сходки встремали,\n",
            "Урегих\n",
            "============================================================\n",
            "Epoch: 4500 | Chunk Loss: 2.2643465169270836\n",
            "Generated text:\n",
            "де, голяба!\n",
            "V\n",
            "\n",
            "Вневет dalfeâ luat\n",
            "II\n",
            "\n",
            "Но», предсторение дрезналаться коней под свете и тва,\n",
            "Гого не пистое, петиньей вдолька во лие, чурит мог и послеб\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "rnn_model = train_rnn(data, n_chars, 512, n_chars, chunk_length=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDMHKjUzL2nD",
        "outputId": "4a5a7ccf-15bc-4a55-8e0c-79d218ff61b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ОнеТет за пошлую\n",
            "С нейта порапов.\n",
            "Поглудает, я головой пит,\n",
            "Они вылдувские брат,\n",
            "Провяла сыв совела\n",
            "И Логтые Ненькой с пуле\n",
            "Усня подомам толке. Онегин клон быяшны,\n",
            "Татьяна встречая Колил; тревижу;\n",
            "Не с толки Онегин молво:\n",
            "И писахеенье сколя,\n",
            "И там и говеря...»\n",
            "На в тапиты соседа емут на.\n",
            "Я смоашал молодой\n",
            "И деревне хроманям и лимавай.\n",
            "V\n",
            "\n",
            "Но подскигу  и станали,\n",
            "Для нароны не подошалый завед.\n",
            "XXXIV\n",
            "\n",
            "И чувственно детя света слышат быть в ваши зонос\n",
            "И взолагия ремы!\n",
            "На виром садемать душал, ктося,\n",
            "В меня суссену, душал и рочно как накодет\n",
            "И челсквый мягчивый слову:\n",
            "(Нугой люной,\n",
            "И назтрадает, не нипяла,\n",
            "С перед судненьей и с ниф взор,\n",
            "Яли могуескою льют, покаресты в предногов\n",
            "Пагам под ней,\n",
            "Вашее ночкой рози,\n",
            "Едва в мало, безмосла новый,\n",
            "Про кативною приина с сенною\n",
            "Там дуручела\n",
            "И борот, и низный! Одесеньем\n",
            "И там и венише и приилбонят\n",
            "И в тими по не блесташесто бьи много метредальсь обовит.\n",
            "Она проминшивый черть.\n",
            "XXV\n",
            "\n",
            "«Кто Лену на тет злыманий душой Клибытур,\n",
            "Начему, искался мой не:\n",
            "Еще чт\n"
          ]
        }
      ],
      "source": [
        "print(generate(rnn_model, \"О\", 1_000, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtDN3jPQu4E"
      },
      "source": [
        "**Deep RNN didn't improve the situation drastically, but it didn't do worse and learned more words.**\n",
        "<br>Seriously, model learned a lot of words and even constructions!<br>\n",
        "This is pretty pog, especially we achieved this result so quickly and easily!<br>\n",
        "Hopefully, this was helpful to u.<br>\n",
        "In the next notebook we will compare **RNN**, **LSTM** and **GRU** to find the best performing model, then we will use word embeddings and train the best model possible!<br>\n",
        "But this is it here!<br>\n",
        "Oh, and let's have a tradition of coolness rate in every miniproject!<br>\n",
        "**Coolness rate: 90%**\n",
        "<br>\n",
        "<img src=\"https://static.wikia.nocookie.net/cyberpunk/images/d/de/Johnny_Silverhand_Database_CP2077.png/revision/latest?cb=20231003222545\" width=13%>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
